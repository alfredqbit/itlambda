{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V6E1",
      "authorship_tag": "ABX9TyOjEQW+CphbZbHYa4wHVZVC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alfredqbit/itlambda/blob/main/itlambda_cobaya.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modified massive gravity models: $f(R,\\mathcal{G})+m_g$, $f(R,\\mathcal{G},T)+m_g$\n",
        "\n",
        "Define modified gravity model calculating custom Hubble parameter $H(z)$ given paramters $\\theta=(H_0,\\Omega_m,m_g,\\lambda_N)$.\n",
        "\n",
        "Create class comuting required theoretical observable $H(z,\\theta)$ as defined in Eq. 8 in paper, \"Information-Theoretic $\\Lambda$: A Holographic Analysis of Generalzied $f(R,\\mathcal{G})$ and $f(R,\\mathcal{G},T)$ Massive Gravity\" by A. Sepulveda-Jimenez.\n",
        "\n",
        "Datasets: joint dataset from Desi DR2, Pantheon+ SNIa, Planck CMB 2018 Low-l, and Planck  CMB 2018 High-l plik\n",
        "\n",
        "©copyright 2026. A. Sepulveda-Jimenez"
      ],
      "metadata": {
        "id": "VxHzEh83PMVu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9pPwiFNOPZf"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install cobaya\n",
        "import numpy as np\n",
        "from cobaya.theory import Theory\n",
        "from scipy.integrate import quad\n",
        "\n",
        "class HolographicCosmo(Theory):\n",
        "    \"\"\"\n",
        "    A custom theory class for the f(R) + massive gravity model\n",
        "    described in Sepulveda-Jiménez (2026).\n",
        "    \"\"\"\n",
        "    params = {\n",
        "        \"H0\": None,\n",
        "        \"Omega_m\": None,\n",
        "        \"mg\": None,\n",
        "        \"lambda_N\": None\n",
        "    }\n",
        "\n",
        "    def initialize(self):\n",
        "        \"\"\"Called once to set up the class (e.g., set redshift range).\"\"\"\n",
        "        self.z_grid = np.linspace(0.0, 3.0, 100)\n",
        "\n",
        "    def H_theory(self, z, H0, Omega_m, mg, lambda_N):\n",
        "        \"\"\"Calculates H(z; theta) as per Eq. 8 in the paper.\"\"\"\n",
        "        # H(z; θ) = H0 * sqrt(Ωm(1+z)^3 + (1−Ωm) * [1 + ηN ln(1+z)] * e^−mgz)\n",
        "        term1 = Omega_m * (1 + z)**3\n",
        "        term2 = (1 - Omega_m) * (1 + lambda_N * np.log(1 + z)) * np.exp(-mg * z)\n",
        "        return H0 * np.sqrt(term1 + term2)\n",
        "\n",
        "    def H_theory_fRGT(z, H0, Omega_m, mg, lambda_N, beta):\n",
        "      \"\"\"\n",
        "      Modified H(z) for the f(R, G, T) extension.\n",
        "      beta: Curvature-matter coupling constant (particle creation rate).\n",
        "     \"\"\"\n",
        "      # Modified matter scaling due to non-conservation\n",
        "      term_matter = Omega_m * (1 + z)**(3 - beta)\n",
        "      # Information-theoretic dark energy sector (from Eq. 5 & 8)\n",
        "      term_de = (1 - Omega_m) * (1 + lambda_N * np.log(1 + z)) * np.exp(-mg * z)\n",
        "      return H0 * np.sqrt(term_matter + term_de)\n",
        "\n",
        "    def H_theory_RTQFT(z, H0, Omega_m, mg, lambda_N, beta, xi_topo):\n",
        "      \"\"\"\n",
        "      Modified H(z) for a RTQFT-inspired topological correction xi_topo.\n",
        "      xi_topo: Represents the influence of the Euler characteristic\n",
        "      on the early-universe expansion rate.\n",
        "      \"\"\"\n",
        "    # Standard f(R,G,T) matter and DE sectors\n",
        "      term_matter = Omega_m * (1 + z)**(3 - beta)\n",
        "      term_de = (1 - Omega_m) * (1 + lambda_N * np.log(1 + z)) * np.exp(-mg * z)\n",
        "    # RTQFT topological correction (significant at high z)\n",
        "      term_topo = xi_topo * (1 + z)**4 * np.exp(-1 / (1 + z))\n",
        "      return H0 * np.sqrt(term_matter + term_de + term_topo)\n",
        "\n",
        "    def calculate(self, state, want_derived=True, **params_values_dict):\n",
        "        \"\"\"Called for each parameter set during sampling.\"\"\"\n",
        "        p = params_values_dict\n",
        "\n",
        "        # Calculate H(z) on the predefined grid\n",
        "        H_values = self.H_theory(self.z_grid, p[\"H0\"], p[\"Omega_m\"], p[\"mg\"], p[\"lambda_N\"])\n",
        "\n",
        "        # Store results for the likelihoods to access\n",
        "        state[\"H_grid\"] = H_values\n",
        "        state[\"z_grid\"] = self.z_grid\n",
        "        # Store params for on-the-fly integration in get_... methods\n",
        "        state[\"params\"] = p\n",
        "\n",
        "    def get_Hubble(self, z, units=\"km/s/Mpc\"):\n",
        "        \"\"\"Returns H(z) in km/s/Mpc. Required by BAO likelihoods.\"\"\"\n",
        "        # The likelihood calls this with units=\"km/s/Mpc\"\n",
        "        return np.interp(z, self.current_state[\"z_grid\"], self.current_state[\"H_grid\"])\n",
        "\n",
        "    def _get_comoving_distance(self, z):\n",
        "        \"\"\"Helper to calculate comoving distance D_M(z) = c * integral(1/H).\"\"\"\n",
        "        c_light = 299792.458 # km/s\n",
        "        p = self.current_state[\"params\"]\n",
        "\n",
        "        def H_inv(zi):\n",
        "            return 1.0 / self.H_theory(zi, p[\"H0\"], p[\"Omega_m\"], p[\"mg\"], p[\"lambda_N\"])\n",
        "\n",
        "        # Handle both scalar and array inputs for z\n",
        "        if np.ndim(z) == 0:\n",
        "            return c_light * quad(H_inv, 0, z)[0]\n",
        "        else:\n",
        "            return np.array([c_light * quad(H_inv, 0, zi)[0] for zi in z])\n",
        "\n",
        "    def get_angular_diameter_distance(self, z, units=\"Mpc\"):\n",
        "        \"\"\"Returns D_A(z) in Mpc. Required by BAO likelihoods.\"\"\"\n",
        "        # D_A = D_M / (1+z)\n",
        "        return self._get_comoving_distance(z) / (1 + z)\n",
        "\n",
        "    def get_luminosity_distance(self, z, units=\"Mpc\"):\n",
        "        \"\"\"Returns D_L(z) in Mpc. Required by SN likelihoods.\"\"\"\n",
        "        # D_L = D_M * (1+z)\n",
        "        return self._get_comoving_distance(z) * (1 + z)\n",
        "\n",
        "def compute_wic(chi_sq, num_params, n_data):\n",
        "    \"\"\"\n",
        "    Compute Weighted Information Criterion (WIC).\n",
        "    Formula: chi_sq + (2 * num_params * n_data) / (n_data - num_params - 1)\n",
        "    \"\"\"\n",
        "    if n_data - num_params - 1 <= 0:\n",
        "        return np.inf\n",
        "    return chi_sq + (2 * num_params * n_data) / (n_data - num_params - 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e348c8c"
      },
      "source": [
        "## Explanation of Results\n",
        "\n",
        "### 1. Triangle Plot (Parameter Constraints)\n",
        "The triangle plot above visualizes the posterior distributions of the model parameters:\n",
        "- **Diagonal Panels (1D Distributions):** These show the probability density for each single parameter.\n",
        "    - **$H_0$ (Hubble Constant):** The peak is at $\\approx 52$ km/s/Mpc, far lower than the standard value ($67-73$). The distribution is cut off at the lower end, suggesting the data wants an even lower value.\n",
        "    - **$r_{\\mathrm{drag}}$ (Sound Horizon):** This \"nuisance\" parameter is pushed to $< 113$ Mpc, which is unphysically low (standard physics predicts $\\approx 147$ Mpc). This indicates the model cannot fit the BAO scale without breaking standard early-universe physics.\n",
        "    - **$m_g$ & $\\lambda_N$:** These parameters show broad distributions, meaning the data does not strongly constrain specific values for the modified gravity modifications, other than preferring regions that minimize the tension (but fail to solve it).\n",
        "\n",
        "- **Off-Diagonal Panels (2D Contours):** These show the correlations between pairs of parameters.\n",
        "    - The strong correlations (e.g., between $H_0$ and $\\Omega_m$) reveal \"degeneracies,\" where changing one parameter can be partially compensated by changing another. The tight, curved shapes indicate that the model is confined to a thin valley of \"least worst\" fits.\n",
        "\n",
        "### 2. Expansion History Plot ($H(z)$ vs $\\Lambda$CDM)\n",
        "This plot compares the best-fit Holographic model (blue) to the standard $\\Lambda$CDM model (red dashed):\n",
        "\n",
        "- **Top Panel ($H(z)$):**\n",
        "    - The Holographic model starts at a much lower $H_0$ ($z=0$) but evolves differently.\n",
        "    - It tries to match the shape required by Supernovae (luminosity distance) while being constrained by the fixed points of the BAO data.\n",
        "\n",
        "- **Bottom Panel (Residuals):**\n",
        "    - This shows the fractional difference: $\\frac{H_{\\mathrm{holo}} - H_{\\Lambda CDM}}{H_{\\Lambda CDM}}$.\n",
        "    - The residuals are non-zero and vary with redshift, confirming that the Holographic model predicts a distinctly different cosmic expansion history.\n",
        "    - The fact that the residuals are significant (and the fit quality $\\chi^2$ is poor) means this specific modified gravity model cannot mimic $\\Lambda$CDM well enough to satisfy current precision data."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from cobaya.run import run\n",
        "import getdist.plots\n",
        "from getdist import loadMCSamples\n",
        "import os\n",
        "\n",
        "# Install external likelihoods\n",
        "\n",
        "!cobaya-install bao.desi_dr2 -p ./packages\n",
        "!cobaya-install sn.pantheonplus -p ./packages\n",
        "\n",
        "# NOTE: Full Planck likelihoods require a Boltzmann code (perturbations).\n",
        "# Since HolographicCosmo is background-only, we cannot use them directly.\n",
        "# !cobaya-install planck_2018_lowl.TT -p ./packages\n",
        "# !cobaya-install planck_2018_highl_plik.TTTEEE -p ./packages\n",
        "\n",
        "# Set the packages path environment variable\n",
        "os.environ[\"COBAYA_PACKAGES_PATH\"] = \"./packages\"\n",
        "\n",
        "info = {\n",
        "    \"likelihood\": {\n",
        "        # Real likelihoods supported by background geometry\n",
        "        \"bao.desi_dr2\": None,\n",
        "        \"sn.pantheonplus\": None,\n",
        "\n",
        "        # FULL Planck likelihoods (require Cls) -> DISABLED\n",
        "        # \"planck_2018_lowl.TT\": None,\n",
        "        # \"planck_2018_highl_plik.TTTEEE\": None,\n",
        "    },\n",
        "    \"theory\": {\n",
        "        # Reference the class object directly\n",
        "        \"HolographicCosmo\": {\"external\": HolographicCosmo}\n",
        "    },\n",
        "    \"params\": {\n",
        "        # Widened priors to address boundary issues from previous run\n",
        "        \"H0\": {\"prior\": {\"min\": 40, \"max\": 90}, \"ref\": 60.0},\n",
        "        \"Omega_m\": {\"prior\": {\"min\": 0.01, \"max\": 0.6}, \"ref\": 0.1},\n",
        "        \"mg\": {\"prior\": {\"min\": 0, \"max\": 3.0}, \"ref\": 0.5, \"latex\": r\"m_g\"},\n",
        "        \"lambda_N\": {\"prior\": {\"min\": 0, \"max\": 1.0}, \"ref\": 0.0, \"latex\": r\"\\lambda_N\"},\n",
        "        # Widened rdrag prior\n",
        "        \"rdrag\": {\"prior\": {\"min\": 100, \"max\": 160}, \"ref\": 147.0, \"latex\": r\"r_\\mathrm{drag}\"}\n",
        "    },\n",
        "    \"sampler\": {\n",
        "        \"mcmc\": {\n",
        "            \"max_samples\": 1000,\n",
        "            \"max_tries\": 5000,\n",
        "            \"proposal_scale\": 1.9\n",
        "        }\n",
        "    },\n",
        "    \"output\": \"chains/custom_holographic_fit\"\n",
        "}\n",
        "\n",
        "# Run the sampler\n",
        "updated_info, sampler = run(info, force=True)\n",
        "\n",
        "# --- Visualization ---\n",
        "gd_sample = loadMCSamples(\"chains/custom_holographic_fit\", settings={'ignore_rows': 0.1})\n",
        "g = getdist.plots.get_subplot_plotter()\n",
        "g.triangle_plot(gd_sample, [\"H0\", \"Omega_m\", \"mg\", \"lambda_N\", \"rdrag\"], filled=True)"
      ],
      "metadata": {
        "id": "H432onzhOoiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis 1: Parameter Constraints Table"
      ],
      "metadata": {
        "id": "VY6ePbjug9at"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97054aa9"
      },
      "source": [
        "from getdist import loadMCSamples\n",
        "from IPython.display import display, Latex\n",
        "\n",
        "# Load the chain results\n",
        "samples = loadMCSamples(\"chains/custom_holographic_fit\", settings={'ignore_rows': 0.3})\n",
        "\n",
        "# Print the constraints table (LaTeX format rendered)\n",
        "# showing mean +/- 1 sigma\n",
        "stats = samples.getMargeStats()\n",
        "table_tex = samples.getTable(limit=1).tableTex()\n",
        "display(Latex(table_tex))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis 2: Best Fit Values & Model Visualization"
      ],
      "metadata": {
        "id": "METdLQ6WguLQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41bc4e99"
      },
      "source": [
        "# --- Analysis 2 & 3: Best Fit Values & Model Visualization ---\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract mean parameters from the chain\n",
        "stats = samples.getMargeStats()\n",
        "H0_mean = stats.parWithName(\"H0\").mean\n",
        "Om_mean = stats.parWithName(\"Omega_m\").mean\n",
        "mg_mean = stats.parWithName(\"mg\").mean\n",
        "lN_mean = stats.parWithName(\"lambda_N\").mean\n",
        "\n",
        "print(f\"Mean Parameters:\\nH0 = {H0_mean:.2f}\\nOmega_m = {Om_mean:.3f}\\nmg = {mg_mean:.3f}\\nlambda_N = {lN_mean:.3f}\")\n",
        "\n",
        "# Instantiate the theory model\n",
        "model = HolographicCosmo()\n",
        "model.initialize()\n",
        "\n",
        "# Calculate H(z) for the Holographic model using mean parameters\n",
        "z_plot = np.linspace(0, 2.5, 100)\n",
        "H_holo = model.H_theory(z_plot, H0_mean, Om_mean, mg_mean, lN_mean)\n",
        "\n",
        "# Calculate H(z) for standard LambdaCDM for comparison (mg=0, lambda_N=0)\n",
        "H_lcdm = model.H_theory(z_plot, H0_mean, Om_mean, 0.0, 0.0)\n",
        "\n",
        "# --- Plotting with Residuals ---\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8), sharex=True, gridspec_kw={'height_ratios': [3, 1]})\n",
        "plt.subplots_adjust(hspace=0.05)\n",
        "\n",
        "# Top Panel: H(z)\n",
        "ax1.plot(z_plot, H_holo, label=rf'Holographic Model ($m_g={mg_mean:.2f}, \\lambda_N={lN_mean:.2f}$)', color='blue', lw=2)\n",
        "ax1.plot(z_plot, H_lcdm, label=r'$\\Lambda$CDM Reference ($m_g=0, \\lambda_N=0$)', color='red', linestyle='--', alpha=0.7)\n",
        "ax1.set_ylabel('$H(z)$ [km/s/Mpc]', fontsize=12)\n",
        "ax1.set_title(r'Expansion History: Best-Fit Holographic Model vs $\\Lambda$CDM', fontsize=14)\n",
        "ax1.legend(fontsize=11)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Bottom Panel: Residuals (Fractional Difference)\n",
        "# (H_holo - H_lcdm) / H_lcdm\n",
        "residuals = (H_holo - H_lcdm) / H_lcdm\n",
        "ax2.plot(z_plot, residuals, color='black', lw=1.5)\n",
        "ax2.axhline(0, color='red', linestyle='--', alpha=0.5)\n",
        "ax2.set_xlabel('Redshift $z$', fontsize=12)\n",
        "ax2.set_ylabel(r'$\\frac{\\Delta H}{H_{\\Lambda CDM}}$', fontsize=12)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06cd931b"
      },
      "source": [
        "# --- Model Comparison: LambdaCDM vs Holographic ---\n",
        "from cobaya.run import run\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from getdist import loadMCSamples\n",
        "\n",
        "print(\"--- Running LambdaCDM Minimization ---\")\n",
        "updated_info_lcdm, sampler_lcdm = run(info, force=True)\n",
        "\n",
        "# 2. Retrieve Statistics\n",
        "# A. Holographic Model (Dynamic extraction from chain)\n",
        "print(\"--- Extracting Holographic Stats ---\")\n",
        "# Load the chain\n",
        "holo_samples = loadMCSamples(\"chains/custom_holographic_fit\", settings={'ignore_rows': 0.3})\n",
        "# Get likelihood statistics\n",
        "like_stats = holo_samples.getLikeStats()\n",
        "# chi2 = 2 * (-log(Likelihood))\n",
        "# logLike_sample in GetDist is -ln(L)\n",
        "chi2_holo = 2 * like_stats.logLike_sample\n",
        "k_holo = 5  # H0, Om, mg, lN, rdrag\n",
        "print(f\"Best-fit Chi2 (Holographic): {chi2_holo:.2f}\")\n",
        "\n",
        "# B. LambdaCDM Stats (from minimization)\n",
        "try:\n",
        "    # Read the .minimum file generated by Cobaya\n",
        "    min_results = pd.read_csv(\"chains/lcdm_ref.minimum\", sep=r'\\s+', comment='#')\n",
        "    chi2_lcdm = min_results['chi2'].iloc[0]\n",
        "except Exception as e:\n",
        "    print(f\"Warning: Could not read minimum file ({e}). Using approximation.\")\n",
        "    chi2_lcdm = 18837.9 # Fallback\n",
        "\n",
        "k_lcdm = 3\n",
        "\n",
        "# N Data\n",
        "N_data = 1715\n",
        "\n",
        "# 3. Calculate Criteria\n",
        "def calculate_criteria(chi2, k, N):\n",
        "    aic = chi2 + 2 * k\n",
        "    bic = chi2 + k * np.log(N)\n",
        "    wic = compute_wic(chi2, k, N)\n",
        "    return aic, bic, wic\n",
        "\n",
        "aic_holo, bic_holo, wic_holo = calculate_criteria(chi2_holo, k_holo, N_data)\n",
        "aic_lcdm, bic_lcdm, wic_lcdm = calculate_criteria(chi2_lcdm, k_lcdm, N_data)\n",
        "\n",
        "# 4. Print Comparison\n",
        "print(f\"\\n{'Model':<15} | {'Chi2':<10} | {'k':<3} | {'AIC':<10} | {'BIC':<10} | {'WIC':<10}\")\n",
        "print(\"-\"*75)\n",
        "print(f\"{'Holographic':<15} | {chi2_holo:<10.1f} | {k_holo:<3} | {aic_holo:<10.1f} | {bic_holo:<10.1f} | {wic_holo:<10.1f}\")\n",
        "print(f\"{'LambdaCDM':<15} | {chi2_lcdm:<10.1f} | {k_lcdm:<3} | {aic_lcdm:<10.1f} | {bic_lcdm:<10.1f} | {wic_lcdm:<10.1f}\")\n",
        "\n",
        "delta_aic = aic_holo - aic_lcdm\n",
        "delta_wic = wic_holo - wic_lcdm\n",
        "\n",
        "print(f\"\\nDelta AIC (Holo - LCDM) = {delta_aic:.1f}\")\n",
        "print(f\"Delta WIC (Holo - LCDM) = {delta_wic:.1f}\")\n",
        "\n",
        "print(\"\\n--- Conclusion ---\")\n",
        "if delta_aic < -2:\n",
        "    print(\"AIC: Holographic model is preferred.\")\n",
        "elif delta_aic > 2:\n",
        "    print(\"AIC: LambdaCDM is preferred.\")\n",
        "else:\n",
        "    print(\"AIC: Models are statistically indistinguishable.\")\n",
        "\n",
        "if delta_wic < -2:\n",
        "    print(\"WIC: Holographic model is preferred.\")\n",
        "elif delta_wic > 2:\n",
        "    print(\"WIC: LambdaCDM is preferred.\")\n",
        "else:\n",
        "    print(\"WIC: Models are statistically indistinguishable.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5a57e411"
      },
      "source": [
        "# Calculate and print confidence intervals for all parameters\n",
        "stats = samples.getMargeStats()\n",
        "params_to_check = ['H0', 'Omega_m', 'mg', 'lambda_N']\n",
        "\n",
        "print(\"--- Parameter Confidence Intervals ---\")\n",
        "for param_name in params_to_check:\n",
        "    par = stats.parWithName(param_name)\n",
        "\n",
        "    # GetDist calculates limits for 0.68, 0.95, 0.99 by default\n",
        "    # limits[0] -> 68%, limits[1] -> 95%\n",
        "    limit68 = par.limits[0]\n",
        "    limit95 = par.limits[1]\n",
        "\n",
        "    print(f\"\\nParameter: {param_name}\")\n",
        "    print(f\"  Mean: {par.mean:.4f} ± {par.err:.4f}\")\n",
        "    print(f\"  68% CI: [{limit68.lower:.4f}, {limit68.upper:.4f}]\")\n",
        "    print(f\"  95% CI: [{limit95.lower:.4f}, {limit95.upper:.4f}]\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8218985"
      },
      "source": [
        "from scipy import stats\n",
        "\n",
        "# 1. Calculate Delta Chi2\n",
        "# (LCDM is the null hypothesis/nested model)\n",
        "delta_chi2 = chi2_lcdm - chi2_holo\n",
        "\n",
        "# 2. Determine degrees of freedom difference\n",
        "# Holographic has 2 extra parameters: mg and lambda_N\n",
        "dof_diff = k_holo - k_lcdm\n",
        "\n",
        "print(f\"Chi2 (LambdaCDM):   {chi2_lcdm:.2f}\")\n",
        "print(f\"Chi2 (Holographic): {chi2_holo:.2f}\")\n",
        "print(f\"Delta Chi2:         {delta_chi2:.2f}\")\n",
        "print(f\"Extra Params (DOF): {dof_diff}\")\n",
        "\n",
        "# 3. Calculate Significance (Wilks' Theorem)\n",
        "if delta_chi2 <= 0:\n",
        "    print(\"\\nResult: The Holographic model does not improve the fit compared to LambdaCDM.\")\n",
        "    print(\"Significance: 0.00 σ\")\n",
        "else:\n",
        "    # Calculate p-value: Probability of seeing this delta_chi2 by chance if LCDM were true\n",
        "    p_value = stats.chi2.sf(delta_chi2, df=dof_diff)\n",
        "\n",
        "    # Convert p-value to Gaussian Sigmas (two-tailed equivalent)\n",
        "    # isf = Inverse Survival Function (Inverse 1-CDF)\n",
        "    sigma_significance = stats.norm.isf(p_value / 2)\n",
        "\n",
        "    print(f\"\\nP-value:      {p_value:.4f}\")\n",
        "    print(f\"Significance: {sigma_significance:.2f} σ\")\n",
        "\n",
        "    if sigma_significance < 1:\n",
        "        print(\"-> Conclusion: No statistically significant preference.\")\n",
        "    elif sigma_significance < 3:\n",
        "        print(\"-> Conclusion: Mild preference (Tension), but not significant.\")\n",
        "    else:\n",
        "        print(\"-> Conclusion: Strong evidence for Holographic model.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## $f(R,\\mathcal{G},T)+m_g$ massive gravity model.\n",
        "\n",
        " The MCMC sampling within the physically motivated range (-0.1,0.1) to test for deviations from standard matter conservation ($\\beta=0$).\n"
      ],
      "metadata": {
        "id": "_juYX_X9wGsE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from cobaya.run import run\n",
        "\n",
        "info_frgt = {\n",
        "    \"likelihood\": {\n",
        "        \"bao.desi_dr2\": None,\n",
        "        \"sn.pantheon_plus\": None\n",
        "    },\n",
        "    \"theory\": {\n",
        "        \"fRGT\": {\"external\": \"holographic_fRGT.py:HolographicFRGT\"}\n",
        "    },\n",
        "    \"params\": {\n",
        "        \"H0\": {\"prior\": {\"min\": 60, \"max\": 80}, \"ref\": 67.8},\n",
        "        \"Omega_m\": {\"prior\": {\"min\": 0.1, \"max\": 0.5}, \"ref\": 0.31},\n",
        "        \"mg\": {\"prior\": {\"min\": 0, \"max\": 0.5}, \"ref\": 0.12},\n",
        "        \"lambda_N\": {\"prior\": {\"min\": 0, \"max\": 0.3}, \"ref\": 0.07},\n",
        "        # Coupling parameter beta: beta=0 recovers the f(R,G) model\n",
        "        \"beta\": {\"prior\": {\"min\": -0.1, \"max\": 0.1}, \"ref\": 0.0, \"latex\": r\"\\beta\"}\n",
        "    },\n",
        "    \"sampler\": {\n",
        "        \"mcmc\": {\"max_samples\": 2000, \"Rminus1_stop\": 0.02}\n",
        "    },\n",
        "    \"output\": \"chains/fRGT_analysis\"\n",
        "}\n",
        "\n",
        "updated_info_frgt, sampler = run(info_frgt)"
      ],
      "metadata": {
        "id": "szxMEiutwG75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Relativistic Topological Quantum Field Theoretic (RTQFT)\n",
        "\n",
        "topological correction $\\chi_{topo}$ to $f(R,\\mathcal{G},T) + m_g$ massive gravity model\n",
        "using simulated DESI DR2 binned data\n",
        "\n",
        "To determine if the DESI DR2 data can distinguish between the matter-curvature coupling ($\\beta$\n",
        ") and the RTQFT-inspired topological correction ($\\chi_{topo}$\n",
        "), use Neural Posterior Estimation (NPE)."
      ],
      "metadata": {
        "id": "9fIdbZ8htwoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sbi import utils as utils\n",
        "from sbi.inference import SNPE, prepare_for_sbi, simulate_for_sbi\n",
        "\n",
        "# 1. Define the Simulator (based on your H(z) definitions)\n",
        "def simulator(params):\n",
        "    # Unpack parameters: H0, Om, mg, lambdaN, beta, xi_topo\n",
        "    H0, Om, mg, lN, beta, xi = params\n",
        "    z = torch.tensor([0.1, 0.5, 1.0, 1.5, 2.0, 2.5]) # Example DESI bins\n",
        "\n",
        "    # f(R,G,T) + RTQFT physics\n",
        "    term_m = Om * (1 + z)**(3 - beta)\n",
        "    term_de = (1 - Om) * (1 + lN * torch.log(1 + z)) * torch.exp(-mg * z)\n",
        "    term_rtqft = xi * (1 + z)**4 * torch.exp(-1 / (1 + z))\n",
        "\n",
        "    H_z = H0 * torch.sqrt(term_m + term_de + term_rtqft)\n",
        "    # Return H(z) with added Gaussian noise to simulate DESI uncertainties\n",
        "    return H_z + torch.randn(len(z)) * 0.5\n",
        "\n",
        "# 2. Define Priors\n",
        "# [H0, Om, mg, lN, beta, xi]\n",
        "prior = utils.BoxUniform(low=torch.tensor([60., 0.1, 0., 0., -0.1, -0.05]),\n",
        "                         high=torch.tensor([80., 0.5, 0.5, 0.3, 0.1, 0.05]))\n",
        "\n",
        "# 3. Prepare for Neural Inference\n",
        "simulator, prior = prepare_for_sbi(simulator, prior)\n",
        "inference = SNPE(prior=prior)\n",
        "\n",
        "# 4. Generate Training Data (e.g., 5000 simulations)\n",
        "theta, x = simulate_for_sbi(simulator, proposal=prior, num_simulations=5000)\n",
        "density_estimator = inference.append_simulations(theta, x).train()\n",
        "posterior = inference.build_posterior(density_estimator)"
      ],
      "metadata": {
        "id": "-7GtdApbtw8g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}